[**ä¸­æ–‡è¯´æ˜**](./README.md) | [**English**](./readme_en.md)

<p align="center">
    <img src="./pics/banner.svg" width="500"/>
</p>

æœ¬é¡¹ç›®æä¾›äº†é¢å‘ä¸­æ–‡çš„BERTé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨ä¸°å¯Œä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†èµ„æºï¼Œæä¾›å¤šå…ƒåŒ–çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©ã€‚
æˆ‘ä»¬æ¬¢è¿å„ä½ä¸“å®¶å­¦è€…ä¸‹è½½ä½¿ç”¨ï¼Œå¹¶å…±åŒä¿ƒè¿›å’Œå‘å±•ä¸­æ–‡èµ„æºå»ºè®¾ã€‚

æœ¬é¡¹ç›®åŸºäºè°·æ­Œå®˜æ–¹BERTï¼šhttps://github.com/google-research/bert

å…¶ä»–ç›¸å…³èµ„æºï¼š
- ä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹ï¼šhttps://github.com/ymcui/Chinese-BERT-wwm


## æ–°é—»
**2023/5/9 ä¿®å¤ä¸‹è½½é“¾æ¥**


<details>
<summary>å†å²æ–°é—»</summary>
2021/2/6 æ‰€æœ‰æ¨¡å‹å·²æ”¯æŒPytorchå’ŒTensorflow1ä»¥åŠTensorflow2ï¼Œè¯·é€šè¿‡transformersåº“è¿›è¡Œè°ƒç”¨æˆ–ä¸‹è½½ã€‚https://huggingface.co/
2021/2/6 æœ¬ç›®å½•å‘å¸ƒçš„æ¨¡å‹æœªæ¥å¯æ¥å…¥[Huggingface-Transformers](https://github.com/huggingface/transformers)ï¼ŒæŸ¥çœ‹[å¿«é€ŸåŠ è½½](#å¿«é€ŸåŠ è½½)
2021/2/6 `bert_12L_cn`å·²å¯ä¸‹è½½ï¼ŒæŸ¥çœ‹[æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
2021/2/6 æä¾›äº†åœ¨å°è§„æ¨¡é€šç”¨è¯­æ–™ï¼ˆ12.5MBï¼‰ä¸Šè®­ç»ƒçš„ä¸­æ–‡`bert_12L_cn`æ¨¡å‹ï¼ŒæŸ¥çœ‹[æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
</details>

## å†…å®¹å¯¼å¼•

| ç« èŠ‚ | æè¿° |
|-|-|
| [ç®€ä»‹](#ç®€ä»‹) | ä»‹ç»BERT-wwmåŸºæœ¬åŸç† |
| [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½) | æä¾›äº†ä¸­æ–‡é¢„è®­ç»ƒBERTä¸‹è½½åœ°å€ |
| [åŸºçº¿ç³»ç»Ÿæ•ˆæœ](#åŸºçº¿ç³»ç»Ÿæ•ˆæœ) | åˆ—ä¸¾äº†éƒ¨åˆ†åŸºçº¿ç³»ç»Ÿæ•ˆæœ |
| [é¢„è®­ç»ƒç»†èŠ‚](#é¢„è®­ç»ƒç»†èŠ‚) | é¢„è®­ç»ƒç»†èŠ‚çš„ç›¸å…³æè¿° |
| [ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒç»†èŠ‚](#ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒç»†èŠ‚) | ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒç»†èŠ‚çš„ç›¸å…³æè¿° |
| [FAQ](#faq) | å¸¸è§é—®é¢˜ç­”ç–‘ |
| [å¼•ç”¨](#å¼•ç”¨) | æœ¬ç›®å½•çš„æŠ€æœ¯æŠ¥å‘Š |

## ç®€ä»‹

**Whole Word Masking (wwm)**ï¼Œæš‚ç¿»è¯‘ä¸º`å…¨è¯Mask`æˆ–`æ•´è¯Mask`ï¼Œæ˜¯è°·æ­Œåœ¨2019å¹´5æœˆ31æ—¥å‘å¸ƒçš„ä¸€é¡¹BERTçš„å‡çº§ç‰ˆæœ¬ï¼Œä¸»è¦æ›´æ”¹äº†åŸé¢„è®­ç»ƒé˜¶æ®µçš„è®­ç»ƒæ ·æœ¬ç”Ÿæˆç­–ç•¥ã€‚
ç®€å•æ¥è¯´ï¼ŒåŸæœ‰åŸºäºWordPieceçš„åˆ†è¯æ–¹å¼ä¼šæŠŠä¸€ä¸ªå®Œæ•´çš„è¯åˆ‡åˆ†æˆè‹¥å¹²ä¸ªå­è¯ï¼Œåœ¨ç”Ÿæˆè®­ç»ƒæ ·æœ¬æ—¶ï¼Œè¿™äº›è¢«åˆ†å¼€çš„å­è¯ä¼šéšæœºè¢«maskã€‚
åœ¨`å…¨è¯Mask`ä¸­ï¼Œå¦‚æœä¸€ä¸ªå®Œæ•´çš„è¯çš„éƒ¨åˆ†WordPieceå­è¯è¢«maskï¼Œåˆ™åŒå±è¯¥è¯çš„å…¶ä»–éƒ¨åˆ†ä¹Ÿä¼šè¢«maskï¼Œå³`å…¨è¯Mask`ã€‚

**éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„maskæŒ‡çš„æ˜¯å¹¿ä¹‰çš„maskï¼ˆæ›¿æ¢æˆ[MASK]ï¼›ä¿æŒåŸè¯æ±‡ï¼›éšæœºæ›¿æ¢æˆå¦å¤–ä¸€ä¸ªè¯ï¼‰ï¼Œå¹¶éåªå±€é™äºå•è¯æ›¿æ¢æˆ`[MASK]`æ ‡ç­¾çš„æƒ…å†µã€‚
æ›´è¯¦ç»†çš„è¯´æ˜åŠæ ·ä¾‹è¯·å‚è€ƒï¼š[#4](https://github.com/ymcui/Chinese-BERT-wwm/issues/4)**

åŒç†ï¼Œç”±äºè°·æ­Œå®˜æ–¹å‘å¸ƒçš„`BERT-base, Chinese`ä¸­ï¼Œä¸­æ–‡æ˜¯ä»¥**å­—**ä¸ºç²’åº¦è¿›è¡Œåˆ‡åˆ†ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ä¼ ç»ŸNLPä¸­çš„ä¸­æ–‡åˆ†è¯ï¼ˆCWSï¼‰ã€‚
æˆ‘ä»¬å°†å…¨è¯Maskçš„æ–¹æ³•åº”ç”¨åœ¨äº†ä¸­æ–‡ä¸­ï¼Œä½¿ç”¨äº†ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆåŒ…æ‹¬ç®€ä½“å’Œç¹ä½“ï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”ä½¿ç”¨äº†åˆ†è¯å·¥å…·ï¼Œå³å¯¹ç»„æˆåŒä¸€ä¸ª**è¯**çš„æ±‰å­—å…¨éƒ¨è¿›è¡ŒMaskã€‚

ä¸‹è¿°æ–‡æœ¬å±•ç¤ºäº†`å…¨è¯Mask`çš„ç”Ÿæˆæ ·ä¾‹ã€‚
**æ³¨æ„ï¼šä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œä¸‹è¿°ä¾‹å­ä¸­åªè€ƒè™‘æ›¿æ¢æˆ[MASK]æ ‡ç­¾çš„æƒ…å†µã€‚**

| è¯´æ˜ | æ ·ä¾‹ |
| :------- | :--------- |
| åŸå§‹æ–‡æœ¬ | ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„probabilityã€‚ |
| åˆ†è¯æ–‡æœ¬ | ä½¿ç”¨ è¯­è¨€ æ¨¡å‹ æ¥ é¢„æµ‹ ä¸‹ ä¸€ä¸ª è¯ çš„ probability ã€‚ |
| åŸå§‹Maskè¾“å…¥ | ä½¿ ç”¨ è¯­ è¨€ [MASK] å‹ æ¥ [MASK] æµ‹ ä¸‹ ä¸€ ä¸ª è¯ çš„ pro [MASK] ##lity ã€‚ |
| å…¨è¯Maskè¾“å…¥ | ä½¿ ç”¨ è¯­ è¨€ [MASK] [MASK] æ¥ [MASK] [MASK] ä¸‹ ä¸€ ä¸ª è¯ çš„ [MASK] [MASK] [MASK] ã€‚ |



## æ¨¡å‹ä¸‹è½½
| æ•°æ®é›†                             | owner  | model             | è¯­è¨€ | å±‚æ•° | å‚æ•°é‡ |
|------------------------------------|--------|-------------------|------|------|--------|
| ä¸­å­¦é˜…è¯»ç†è§£                       | Brian Shen | [bert_3L_cn](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert_3L_cn.tgz)        | cn   | 3    | 27.5M  |
| ä¸­å­¦é˜…è¯»ç†è§£                       | Brian Shen | [bert_6L_cn](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert_6L_cn.tgz)        | cn   | 6    | 55M    |
| ä¸­æ–‡ç»´åŸº                           | Google | [bert_12L_cn-alpha](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert_12L_cn-alpha.tgz) | cn   | 12   | 110M   |
| ä¸­æ–‡ç»´åŸº                           | Google | [bert_12L_cn-beta](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert_12L_cn-beta.tgz)  | cn   | 12   | 110M   |
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­” | Brian Shen | [bert-3L_cn-alpha](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert-3L_cn-alpha.tgz)  | cn   | 3    | 27.5M  |
| ä¸­å­¦é˜…è¯»ç†è§£                       | Brian Shen | [bert-3L_cn-beta](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert-3L_cn-beta.tgz)   | cn   | 3    | 27.5M  |
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­” | Brian Shen | [bert_12L_cn](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert_12L_cn.tgz)       | cn   | 12   | 110M   |
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­” | Brian Shen | [bert_24L_cn-alpha](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert_24L_cn-alpha.tgz) | cn   | 24   | 330M   |
| QA                                 | Brian Shen | [bert_24L_cn-beta](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert_24L_cn-beta.tgz)  | cn   | 24   | 330M   |
| QA                                 | Brian Shen | [bert_24L_cn-gamma](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//bert_24L_cn-gamma.tgz) | cn   | 24   | 330M   |
| QA                                 | Brian Shen | [xlnet_6L_cn](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//xlnet_6L_cn.tgz)       | cn   | 6    | 53.5M  |
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­” | Brian Shen | [xlnet_12L_cn](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//xlnet_12L_cn.tgz)      | cn   | 12   | 117M   |
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­” | Brian Shen | [xlnet_24L_cn](https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt//xlnet_24L_cn.tgz)      | cn   | 24   | 209M   |


> **`base`**ï¼š12-layer, 768-hidden, 12-heads, 110M parameters()  
> **`large`**ï¼š24-layer, 1024-hidden, 16-heads, 330M parameters()
> 
> [1] é€šç”¨æ•°æ®åŒ…æ‹¬ï¼šé—®ç­”ç­‰æ•°æ®ï¼Œæ€»å¤§å°12.5MBï¼Œè®°å½•æ•°1ä¸‡ï¼Œå­—æ•°7.2ä¸‡ã€‚  
> [2] åŠ è½½pytorchå’Œtf2æ¨¡å‹æ—¶ï¼Œå¦‚transformersåŠ è½½æŠ¥xlaé”™è¯¯ï¼Œè¯·è‡ªè¡Œä¿®æ”¹config.jsonä¸­`xla_device`çš„å€¼ï¼Œå¦‚åœ¨gpuä¸Šå¾®è°ƒéœ€è¦è®¾ä¸ºfalseï¼Œå¦‚åœ¨tpuä¸Šå¾®è°ƒï¼Œåˆ™éœ€è¦è®¾ä¸ºtrueã€‚


### PyTorch/Tensorflowç‰ˆæœ¬

æä¾›PyTorchç‰ˆæœ¬ã€‚

### ä½¿ç”¨è¯´æ˜

`bert_12L_cn`æ¨¡å‹æ–‡ä»¶å¤§å°çº¦**454M**å’Œ**1.3G**ã€‚

Pytorchç‰ˆæœ¬ä¸ºï¼š

```
chinese_BERT_base_L-12_H-768_A-12.zip
    |- pytorch_model.bin     # æ¨¡å‹æƒé‡
    |- config.json           # æ¨¡å‹å‚æ•°
    |- training_args.bin     # æ¨¡å‹è®­ç»ƒä¿¡æ¯
    |- vocab.txt             # åˆ†è¯è¯è¡¨
```


### å¿«é€ŸåŠ è½½

ä¾æ‰˜äº[Huggingface-Transformers 3.1.0](https://github.com/huggingface/transformers) ï¼Œå¯è½»æ¾è°ƒç”¨ä»¥ä¸Šæ¨¡å‹ã€‚
```
tokenizer = AutoTokenizer.from_pretrained("MODEL_NAME")
model = AutoModel.from_pretrained("MODEL_NAME")

æˆ–

tokenizer = BertTokenizer.from_pretrained("MODEL_NAME")
model = BertModel.from_pretrained("MODEL_NAME")
```



## åŸºçº¿ç³»ç»Ÿæ•ˆæœ
ä¸ºäº†å¯¹æ¯”åŸºçº¿æ•ˆæœï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹å‡ ä¸ªä¸­æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚å¯¹æ¯”äº†ä¸­æ–‡BERT-wwm-extã€BERT-baseä»¥åŠæœ¬é¡¹ç›®çš„bert_12L_cnã€‚
æ—¶é—´åŠç²¾åŠ›æœ‰é™ï¼Œå¹¶æœªèƒ½è¦†ç›–æ›´å¤šç±»åˆ«çš„ä»»åŠ¡ï¼Œè¯·å¤§å®¶è‡ªè¡Œå°è¯•ã€‚



## é¢„è®­ç»ƒç»†èŠ‚
ä»¥ä¸‹ä»¥`bert_12L_cn`æ¨¡å‹ä¸ºä¾‹ï¼Œå¯¹é¢„è®­ç»ƒç»†èŠ‚è¿›è¡Œè¯´æ˜ã€‚

### ç”Ÿæˆè¯è¡¨

æŒ‰ç…§BERTå®˜æ–¹æ•™ç¨‹æ­¥éª¤ï¼Œé¦–å…ˆéœ€è¦ä½¿ç”¨[Word Piece](https://pypi.org/project/tokenizers/) ç”Ÿæˆè¯è¡¨ã€‚
WordPieceæ˜¯ç”¨äºBERTã€DistilBERTå’ŒElectraçš„å­è¯æ ‡è®°åŒ–ç®—æ³•ã€‚è¯¥ç®—æ³•åœ¨æ—¥è¯­å’ŒéŸ©è¯­è¯­éŸ³æœç´¢ï¼ˆSchuster et al.ï¼Œ2012ï¼‰ä¸­è¿›è¡Œäº†æ¦‚è¿°ï¼Œä¸BPEéå¸¸ç›¸ä¼¼ã€‚WordPieceé¦–å…ˆåˆå§‹åŒ–è¯æ±‡è¡¨ä»¥åŒ…å«è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸ªå­—ç¬¦ï¼Œå¹¶é€æ­¥å­¦ä¹ ç»™å®šæ•°é‡çš„åˆå¹¶è§„åˆ™ã€‚ä¸BPEä¸åŒçš„æ˜¯ï¼ŒWordPieceå¹¶æ²¡æœ‰é€‰æ‹©æœ€é¢‘ç¹çš„ç¬¦å·å¯¹ï¼Œè€Œæ˜¯é€‰æ‹©å°†è®­ç»ƒæ•°æ®æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„å¯èƒ½æ€§æœ€å¤§åŒ–çš„ç¬¦å·å¯¹ã€‚
é‚£ä¹ˆè¿™åˆ°åº•æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿå‚ç…§å‰é¢çš„ç¤ºä¾‹ï¼Œæœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„å¯èƒ½æ€§ç›¸å½“äºæ‰¾åˆ°ç¬¦å·å¯¹ï¼Œå…¶æ¦‚ç‡é™¤ä»¥å…¶ç¬¬ä¸€ä¸ªç¬¦å·çš„æ¦‚ç‡ç„¶åå†é™¤ä»¥å…¶ç¬¬äºŒä¸ªç¬¦å·çš„æ¦‚ç‡æ˜¯æ‰€æœ‰ç¬¦å·å¯¹ä¸­æœ€å¤§çš„ã€‚Eã€ åªæœ‰å½“â€œugâ€é™¤ä»¥â€œuâ€ã€â€œgâ€çš„æ¦‚ç‡å¤§äºä»»ä½•å…¶ä»–ç¬¦å·å¯¹æ—¶ï¼Œâ€œuâ€åæ¥â€œgâ€æ‰ä¼šåˆå¹¶ã€‚ç›´è§‚åœ°è¯´ï¼ŒWordPieceä¸BPEç¨æœ‰ä¸åŒï¼Œå®ƒé€šè¿‡åˆå¹¶ä¸¤ä¸ªç¬¦å·æ¥è¯„ä¼°å®ƒæ‰€å¤±å»çš„ä¸œè¥¿ï¼Œä»¥ç¡®ä¿å®ƒæ˜¯å€¼å¾—çš„ã€‚

åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„è¯è¡¨å¤§å°ä¸º21128ï¼Œå…¶ä½™å‚æ•°é‡‡ç”¨å®˜æ–¹ç¤ºä¾‹ä¸­çš„é»˜è®¤é…ç½®ã€‚

```
# Initialize a tokenizer
tokenizer = BertWordPieceTokenizer()
# Customize training
tokenizer.train(files=paths, vocab_size=21_128, min_frequency=0, special_tokens=[
                "[PAD]",
                "[UNK]",
                "[CLS]",
                "[SEP]",
                "[MASK]",
                "<S>",
                "<T>"
                ])
```


### ç”Ÿæˆè¯è¡¨ç®—æ³•

ä¸‹åˆ—æ–¹æ³•å¹¶étokenizersçš„å®ç°ï¼Œä½œä¸ºæ„æ€è¡¨è¾¾ã€‚è¿›ä¸€æ­¥åœ°ï¼Œå¯¹äºä¸€ä¸ªè‹±æ–‡è¯ï¼ˆä¸­æ–‡åˆ†è¯åŒç†ï¼‰ï¼ŒæŒ‰ç…§WPè§„åˆ™ï¼Œå¯åˆ†æˆå¤šä¸ªé«˜é¢‘ç‰‡æ®µã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```
def tokenize(self, text):
  
  # æŠŠä¸€æ®µæ–‡å­—åˆ‡åˆ†æˆword pieceã€‚è¿™å…¶å®æ˜¯è´ªå¿ƒçš„æœ€å¤§æ­£å‘åŒ¹é…ç®—æ³•ã€‚
  # æ¯”å¦‚ï¼š
  # input = "unaffable"
  # output = ["un", "##aff", "##able"]
 
  
  text = convert_to_unicode(text)
  
  output_tokens = []
  for token in whitespace_tokenize(text):
	  chars = list(token)
	  if len(chars) > self.max_input_chars_per_word:
		  output_tokens.append(self.unk_token)
		  continue
	  
	  is_bad = False
	  start = 0
	  sub_tokens = []
	  while start < len(chars):
		  end = len(chars)
		  cur_substr = None
		  while start < end:
			  substr = "".join(chars[start:end])
			  if start > 0:
				  substr = "##" + substr
			  if substr in self.vocab:
				  cur_substr = substr
				  break
			  end -= 1
		  if cur_substr is None:
			  is_bad = True
			  break
		  sub_tokens.append(cur_substr)
		  start = end
	  
	  if is_bad:
		  output_tokens.append(self.unk_token)
	  else:
		  output_tokens.extend(sub_tokens)
  return output_tokens
```



### é¢„è®­ç»ƒ
è·å¾—ä»¥ä¸Šæ•°æ®åï¼Œæˆªæ­¢2021å¹´2æœˆ6æ—¥ï¼Œä½¿ç”¨BERT-wwm-extçš„WordPieceè¯è¡¨ï¼ˆæ¨¡å‹ï¼‰ï¼Œ(æœªæ¥å°†ä½¿ç”¨åŸºäºé€šç”¨æ•°æ®çš„WordPieceæ¨¡å‹)ï¼Œæ­£å¼å¼€å§‹é¢„è®­ç»ƒBERTã€‚
ä¹‹æ‰€ä»¥å«`bert_12L_cn`æ˜¯å› ä¸ºä»…ç›¸æ¯”`BERT-wwm-ext`ï¼Œå…¶ä½™å‚æ•°æ²¡æœ‰å˜åŠ¨ï¼Œä¸»è¦å› ä¸ºè®¡ç®—è®¾å¤‡å—é™ã€‚
ä½¿ç”¨çš„å‘½ä»¤å¦‚ä¸‹ï¼š
```
    from transformers import (
        CONFIG_MAPPING,
        MODEL_WITH_LM_HEAD_MAPPING,
        AutoConfig,
        BertConfig,
        RobertaConfig,
        AutoModelWithLMHead,
        BertForMaskedLM,
        RobertaForMaskedLM,
        AutoTokenizer,
        DataCollatorForLanguageModeling,
        HfArgumentParser,
        LineByLineTextDataset,
        PreTrainedTokenizer,
        TextDataset,
        Trainer,
        TrainingArguments,
        set_seed,
        BertTokenizer
    )
   
    TEMP="temp/"
    
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-c', '--config_file', default='config/bert_config.json',
        help='model config file')
    
    args = parser.parse_args()
    with open(args.config_file) as fin:
        config = json.load(fin, object_hook=lambda d: SimpleNamespace(**d))
    
    bert_config = BertConfig.from_pretrained(config.bert_model_path, cache_dir=TEMP)
    
    WRAPPED_MODEL = BertForMaskedLM.from_pretrained(
                config.bert_model_path,
                from_tf=False,
                config=bert_config,
                cache_dir=TEMP,
            )
    for param in WRAPPED_MODEL.parameters():
        param.requires_grad = True
    WRAPPED_MODEL.train()
    
    tokenizer = BertTokenizer.from_pretrained(config.bert_model_path)
    WRAPPED_MODEL.resize_token_embeddings(len(tokenizer))
    
    print("dataset maxl:", config.max_seq_len)
    
    dataset = LineByLineTextDataset(
        tokenizer=tokenizer,
        file_path=config.train_file_path,
        block_size=config.max_seq_len,
    )
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=0.15
    )
    
    """### Finally, we are all set to initialize our Trainer"""
    training_args = TrainingArguments(
        output_dir=TEMP,
        overwrite_output_dir=True,
        num_train_epochs=1,
        per_device_train_batch_size=config.batch_size,
        save_steps=10_000,
        save_total_limit=2,
        tpu_num_cores=8,
    )
    
    trainer = Trainer(
        model=WRAPPED_MODEL,
        args=training_args,
        data_collator=data_collator,
        train_dataset=dataset,
        prediction_loss_only=True,
    )
    
    
    trainer.train(model_path=config.bert_model_path)
    WRAPPED_MODEL.to('cpu')
    trainer.save_model(output_dir=config.trained_model_path)
    torch.save(WRAPPED_MODEL.state_dict(), os.path.join(config.trained_model_path, 'pytorch_model.bin'))
```

## ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒç»†èŠ‚
ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒä½¿ç”¨çš„è®¾å¤‡æ˜¯è°·æ­ŒCloud GPUï¼ˆ16G HBMï¼‰ï¼Œä»¥ä¸‹ç®€è¦è¯´æ˜å„ä»»åŠ¡ç²¾è°ƒæ—¶çš„é…ç½®ã€‚
**ç›¸å…³ä»£ç è¯·æŸ¥çœ‹é¡¹ç›®ã€‚**


## FAQ
**Q: è¿™ä¸ªæ¨¡å‹æ€ä¹ˆç”¨ï¼Ÿ**  
A: è°·æ­Œå‘å¸ƒçš„ä¸­æ–‡BERTæ€ä¹ˆç”¨ï¼Œè¿™ä¸ªå°±æ€ä¹ˆç”¨ã€‚
**æ–‡æœ¬ä¸éœ€è¦ç»è¿‡åˆ†è¯ï¼Œwwmåªå½±å“é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å½±å“ä¸‹æ¸¸ä»»åŠ¡çš„è¾“å…¥ã€‚**

**Q: è¯·é—®æœ‰é¢„è®­ç»ƒä»£ç æä¾›å—ï¼Ÿ**  
A: å¾ˆé—æ†¾ï¼Œæˆ‘ä¸èƒ½æä¾›ç›¸å…³ä»£ç ï¼Œå®ç°å¯ä»¥å‚è€ƒ [#10](https://github.com/ymcui/Chinese-BERT-wwm/issues/10) å’Œ [#13](https://github.com/ymcui/Chinese-BERT-wwm/issues/13)ã€‚

**Q: æŸæŸæ•°æ®é›†åœ¨å“ªé‡Œä¸‹è½½ï¼Ÿ**  
A: è¯·æŸ¥çœ‹`data`ç›®å½•ï¼Œä»»åŠ¡ç›®å½•ä¸‹çš„`README.md`æ ‡æ˜äº†æ•°æ®æ¥æºã€‚å¯¹äºæœ‰ç‰ˆæƒçš„å†…å®¹ï¼Œè¯·è‡ªè¡Œæœç´¢æˆ–ä¸åŸä½œè€…è”ç³»è·å–æ•°æ®ã€‚

**Q: ä¼šæœ‰è®¡åˆ’å‘å¸ƒæ›´å¤§æ¨¡å‹å—ï¼Ÿæ¯”å¦‚BERT-large-wwmç‰ˆæœ¬ï¼Ÿ**  
A: å¦‚æœæˆ‘ä»¬ä»å®éªŒä¸­å¾—åˆ°æ›´å¥½æ•ˆæœï¼Œä¼šè€ƒè™‘å‘å¸ƒæ›´å¤§çš„ç‰ˆæœ¬ã€‚

**Q: ä½ éª—äººï¼æ— æ³•å¤ç°ç»“æœğŸ˜‚**  
A: åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€ç®€å•çš„æ¨¡å‹ã€‚æ¯”å¦‚åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨çš„æ˜¯`run_classifier.py`ï¼ˆè°·æ­Œæä¾›ï¼‰ã€‚
å¦‚æœæ— æ³•è¾¾åˆ°å¹³å‡å€¼ï¼Œè¯´æ˜å®éªŒæœ¬èº«å­˜åœ¨bugï¼Œè¯·ä»”ç»†æ’æŸ¥ã€‚
æœ€é«˜å€¼å­˜åœ¨å¾ˆå¤šéšæœºå› ç´ ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯èƒ½å¤Ÿè¾¾åˆ°æœ€é«˜å€¼ã€‚
å¦å¤–ä¸€ä¸ªå…¬è®¤çš„å› ç´ ï¼šé™ä½batch sizeä¼šæ˜¾è‘—é™ä½å®éªŒæ•ˆæœï¼Œå…·ä½“å¯å‚è€ƒBERTï¼ŒXLNetç›®å½•çš„ç›¸å…³Issueã€‚

**Q: æˆ‘è®­å‡ºæ¥æ¯”ä½ æ›´å¥½çš„ç»“æœï¼**  
A: æ­å–œä½ ã€‚

**Q: è®­ç»ƒèŠ±äº†å¤šé•¿æ—¶é—´ï¼Œåœ¨ä»€ä¹ˆè®¾å¤‡ä¸Šè®­ç»ƒçš„ï¼Ÿ**  
A: è®­ç»ƒæ˜¯åœ¨è°·æ­ŒTPU v3ç‰ˆæœ¬ï¼ˆ128G HBMï¼‰å®Œæˆçš„ï¼Œè®­ç»ƒBERT-wwm-baseèŠ±è´¹çº¦4å°æ—¶ï¼ŒBERT-wwm-largeåˆ™èŠ±è´¹çº¦8å°æ—¶ã€‚

**Q: BERT-wwmçš„æ•ˆæœä¸æ˜¯åœ¨æ‰€æœ‰ä»»åŠ¡éƒ½å¾ˆå¥½**  
A: æœ¬é¡¹ç›®çš„ç›®çš„æ˜¯ä¸ºç ”ç©¶è€…æä¾›å¤šå…ƒåŒ–çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè‡ªç”±é€‰æ‹©BERTï¼ŒERNIEï¼Œæˆ–è€…æ˜¯BERT-wwmã€‚
æˆ‘ä»¬ä»…æä¾›å®éªŒæ•°æ®ï¼Œå…·ä½“æ•ˆæœå¦‚ä½•è¿˜æ˜¯å¾—åœ¨è‡ªå·±çš„ä»»åŠ¡ä¸­ä¸æ–­å°è¯•æ‰èƒ½å¾—å‡ºç»“è®ºã€‚
å¤šä¸€ä¸ªæ¨¡å‹ï¼Œå¤šä¸€ç§é€‰æ‹©ã€‚

**Q: ä¸ºä»€ä¹ˆæœ‰äº›æ•°æ®é›†ä¸Šæ²¡æœ‰è¯•ï¼Ÿ**  
A: å¾ˆå¦ç‡çš„è¯´ï¼š
1ï¼‰æ²¡ç²¾åŠ›æ‰¾æ›´å¤šçš„æ•°æ®ï¼›
2ï¼‰æ²¡æœ‰å¿…è¦ï¼› 
3ï¼‰æ²¡æœ‰é’ç¥¨ï¼›

**Q: ç®€å•è¯„ä»·ä¸€ä¸‹è¿™å‡ ä¸ªæ¨¡å‹**  
A: å„æœ‰ä¾§é‡ï¼Œå„æœ‰åƒç§‹ã€‚
ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶å‘å±•éœ€è¦å¤šæ–¹å…±åŒåŠªåŠ›ã€‚

**Q: æ›´å¤šå…³äº`RoBERTa-wwm-ext`æ¨¡å‹çš„ç»†èŠ‚ï¼Ÿ**  
A: æˆ‘ä»¬é›†æˆäº†RoBERTaå’ŒBERT-wwmçš„ä¼˜ç‚¹ï¼Œå¯¹ä¸¤è€…è¿›è¡Œäº†ä¸€ä¸ªè‡ªç„¶çš„ç»“åˆã€‚
å’Œä¹‹å‰æœ¬ç›®å½•ä¸­çš„æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«å¦‚ä¸‹:  
1ï¼‰é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨wwmç­–ç•¥è¿›è¡Œmaskï¼ˆä½†æ²¡æœ‰ä½¿ç”¨dynamic maskingï¼‰  
2ï¼‰ç®€å•å–æ¶ˆNext Sentence Predictionï¼ˆNSPï¼‰loss  
3ï¼‰ä¸å†é‡‡ç”¨å…ˆmax_len=128ç„¶åå†max_len=512çš„è®­ç»ƒæ¨¡å¼ï¼Œç›´æ¥è®­ç»ƒmax_len=512  
4ï¼‰è®­ç»ƒæ­¥æ•°é€‚å½“å»¶é•¿  

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹å¹¶éåŸç‰ˆRoBERTaæ¨¡å‹ï¼Œåªæ˜¯æŒ‰ç…§ç±»ä¼¼RoBERTaè®­ç»ƒæ–¹å¼è®­ç»ƒå‡ºçš„BERTæ¨¡å‹ï¼Œå³RoBERTa-like BERTã€‚
æ•…åœ¨ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ã€æ¨¡å‹è½¬æ¢æ—¶è¯·æŒ‰BERTçš„æ–¹å¼å¤„ç†ï¼Œè€ŒéRoBERTaã€‚


## å¼•ç”¨
å¦‚æœæœ¬ç›®å½•ä¸­çš„å†…å®¹å¯¹ä½ çš„ç ”ç©¶å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿åœ¨è®ºæ–‡ä¸­å¼•ç”¨ä¸‹è¿°æŠ€æœ¯æŠ¥å‘Šï¼š


## è‡´è°¢
é¡¹ç›®ä½œè€…ï¼š Brian Shen. Twitter@dezhou.

å»ºè®¾è¯¥é¡¹ç›®è¿‡ç¨‹ä¸­å‚è€ƒäº†å¦‚ä¸‹ä»“åº“ï¼Œåœ¨è¿™é‡Œè¡¨ç¤ºæ„Ÿè°¢ï¼š
- BERTï¼šhttps://github.com/google-research/bert
- ä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹ï¼šhttps://github.com/ymcui/Chinese-BERT-wwm


## å…è´£å£°æ˜
æœ¬é¡¹ç›®å¹¶é[BERTå®˜æ–¹](https://github.com/google-research/bert) å‘å¸ƒçš„Chinese BERTæ¨¡å‹ã€‚
è¯¥é¡¹ç›®ä¸­çš„å†…å®¹ä»…ä¾›æŠ€æœ¯ç ”ç©¶å‚è€ƒï¼Œä¸ä½œä¸ºä»»ä½•ç»“è®ºæ€§ä¾æ®ã€‚
ä½¿ç”¨è€…å¯ä»¥åœ¨è®¸å¯è¯èŒƒå›´å†…ä»»æ„ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨è¯¥é¡¹ç›®å†…å®¹é€ æˆçš„ç›´æ¥æˆ–é—´æ¥æŸå¤±è´Ÿè´£ã€‚


## å…³æ³¨æˆ‘ä»¬
æ¬¢è¿å…³æ³¨çŸ¥ä¹ä¸“æ å·ã€‚

[æ·±åº¦å­¦ä¹ å…´è¶£å°ç»„](https://www.zhihu.com/column/thuil)


## é—®é¢˜åé¦ˆ & è´¡çŒ®
å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚  
æˆ‘ä»¬æ²¡æœ‰è¿è¥ï¼Œé¼“åŠ±ç½‘å‹äº’ç›¸å¸®åŠ©è§£å†³é—®é¢˜ã€‚  
å¦‚æœå‘ç°å®ç°ä¸Šçš„é—®é¢˜æˆ–æ„¿æ„å…±åŒå»ºè®¾è¯¥é¡¹ç›®ï¼Œè¯·æäº¤Pull Requestã€‚  

é¡¹ç›®ç›¸å…³è”ç³»æ–¹å¼ï¼šçŸ¥ä¹ç§ä¿¡[tsinghuaboy](https://www.zhihu.com/people/tsinghuaboy)
