[**ä¸­æ–‡è¯´æ˜**](./README.md) | [**English**](./readme_en.md)

<p align="center">
    <img src="./pics/banner.svg" width="500"/>
</p>

æœ¬é¡¹ç›®æä¾›äº†é¢å‘ä¸­æ–‡çš„BERTé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨ä¸°å¯Œä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†èµ„æºï¼Œæä¾›å¤šå…ƒåŒ–çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©ã€‚
æ¬¢è¿å„ä½ä¸“å®¶å­¦è€…ä¸‹è½½ä½¿ç”¨ï¼Œå¹¶å…±åŒä¿ƒè¿›å’Œå‘å±•ä¸­æ–‡èµ„æºå»ºè®¾ã€‚

æœ¬é¡¹ç›®åŸºäºè°·æ­Œå®˜æ–¹BERTï¼šhttps://github.com/google-research/bert

å…¶ä»–ç›¸å…³èµ„æºï¼š
- ä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹ï¼šhttps://github.com/ymcui/Chinese-BERT-wwm


## æ–°é—»
**2023/5/27 å‘å¸ƒSentence-BERT:Drogo, Base, Large, Multi-Lingual-Base**
Sentence-Bertä»…ç”¨äºæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºæ—¶æ•ˆæœè¾ƒå¥½ï¼Œå‚è€ƒsentence_transformersé¡¹ç›®ã€‚


<details>
<summary>å†å²æ–°é—»</summary>

2023/5/23 å‘å¸ƒStark-BERT: Eddard, Lyarra, Rickard, Lyanna,åŒ…æ‹¬tfã€pytorchæ¨¡å‹ã€‚
12-layer, 768-hidden, 12-heads, 102.3M/105.1M parameters 
è®­ç»ƒ20ä¸‡æ­¥ã€‚
2023/5/22 å‘å¸ƒå¤œç‹BERT: bert_night-king_36L_cn,åŒ…æ‹¬tfã€pytorchæ¨¡å‹ã€‚ä¸­æ–‡æ¨¡å‹bert_night-kingï¼šå…¶è¯¦ç»†å‚æ•°ä¸º36-layer, 1024-hidden, 16-heads, 476.7M parameters  
2023/5/16 å‘å¸ƒä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹BERT-Tiny-CNï¼ŒBERT-Mini-CNã€‚  
ç”±æ–°é—»è¯­æ–™è®­ç»ƒ100kæ­¥ã€‚è¶…å‚æ•°ä¸è°·æ­ŒBERTåŸºæœ¬ä¸€è‡´ã€‚  
BERT-Tiny: masked_lm_accuracy=22.74%ï¼ŒNSP_accuracy=100%ã€‚  
BERT-Mini: masked_lm_accuracy=33.54%ï¼ŒNSP_accuracy=100%ã€‚  
ä¸Šè¿°åˆ†è¯MASKæ–¹æ³•ä½¿ç”¨è°·æ­Œé»˜è®¤æ–¹æ³•ï¼šåŒºåˆ†å¤§å°å†™ï¼ŒæŒ‰ä¸­æ–‡å­—åˆ†è¯ã€‚
è¯è¡¨é‡‡ç”¨è°·æ­Œä¸­æ–‡é»˜è®¤çš„21128ä¸ªè¯çš„è¯è¡¨ã€‚  

2023/5/9 ä¿®å¤ä¸‹è½½é“¾æ¥
2021/2/6 æ‰€æœ‰æ¨¡å‹å·²æ”¯æŒPytorchå’ŒTensorflow1ä»¥åŠTensorflow2ï¼Œè¯·é€šè¿‡transformersåº“è¿›è¡Œè°ƒç”¨æˆ–ä¸‹è½½ã€‚https://huggingface.co/
2021/2/6 æœ¬ç›®å½•å‘å¸ƒçš„æ¨¡å‹æœªæ¥å¯æ¥å…¥[Huggingface-Transformers](https://github.com/huggingface/transformers)ï¼ŒæŸ¥çœ‹[å¿«é€ŸåŠ è½½](#å¿«é€ŸåŠ è½½)
2021/2/6 `bert_12L_cn`å·²å¯ä¸‹è½½ï¼ŒæŸ¥çœ‹[æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
2021/2/6 æä¾›äº†åœ¨å°è§„æ¨¡é€šç”¨è¯­æ–™ï¼ˆ12.5MBï¼‰ä¸Šè®­ç»ƒçš„ä¸­æ–‡`bert_12L_cn`æ¨¡å‹ï¼ŒæŸ¥çœ‹[æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
</details>

## å†…å®¹å¯¼å¼•

| ç« èŠ‚ | æè¿° |
|-|-|
| [ç®€ä»‹](#ç®€ä»‹) | ä»‹ç»BERT-wwmåŸºæœ¬åŸç† |
| [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½) | æä¾›äº†ä¸­æ–‡é¢„è®­ç»ƒBERTä¸‹è½½åœ°å€ |
| [åŸºçº¿ç³»ç»Ÿæ•ˆæœ](#åŸºçº¿ç³»ç»Ÿæ•ˆæœ) | åˆ—ä¸¾äº†éƒ¨åˆ†åŸºçº¿ç³»ç»Ÿæ•ˆæœ |
| [é¢„è®­ç»ƒç»†èŠ‚](#é¢„è®­ç»ƒç»†èŠ‚) | é¢„è®­ç»ƒç»†èŠ‚çš„ç›¸å…³æè¿° |
| [ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒç»†èŠ‚](#ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒç»†èŠ‚) | ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒç»†èŠ‚çš„ç›¸å…³æè¿° |
| [FAQ](#faq) | å¸¸è§é—®é¢˜ç­”ç–‘ |
| [å¼•ç”¨](#å¼•ç”¨) | æœ¬ç›®å½•çš„æŠ€æœ¯æŠ¥å‘Š |

## ç®€ä»‹

**Whole Word Masking (wwm)**ï¼Œæš‚ç¿»è¯‘ä¸º`å…¨è¯Mask`æˆ–`æ•´è¯Mask`ï¼Œæ˜¯è°·æ­Œåœ¨2019å¹´5æœˆ31æ—¥å‘å¸ƒçš„ä¸€é¡¹BERTçš„å‡çº§ç‰ˆæœ¬ï¼Œä¸»è¦æ›´æ”¹äº†åŸé¢„è®­ç»ƒé˜¶æ®µçš„è®­ç»ƒæ ·æœ¬ç”Ÿæˆç­–ç•¥ã€‚
ç®€å•æ¥è¯´ï¼ŒåŸæœ‰åŸºäºWordPieceçš„åˆ†è¯æ–¹å¼ä¼šæŠŠä¸€ä¸ªå®Œæ•´çš„è¯åˆ‡åˆ†æˆè‹¥å¹²ä¸ªå­è¯ï¼Œåœ¨ç”Ÿæˆè®­ç»ƒæ ·æœ¬æ—¶ï¼Œè¿™äº›è¢«åˆ†å¼€çš„å­è¯ä¼šéšæœºè¢«maskã€‚
åœ¨`å…¨è¯Mask`ä¸­ï¼Œå¦‚æœä¸€ä¸ªå®Œæ•´çš„è¯çš„éƒ¨åˆ†WordPieceå­è¯è¢«maskï¼Œåˆ™åŒå±è¯¥è¯çš„å…¶ä»–éƒ¨åˆ†ä¹Ÿä¼šè¢«maskï¼Œå³`å…¨è¯Mask`ã€‚

**éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„maskæŒ‡çš„æ˜¯å¹¿ä¹‰çš„maskï¼ˆæ›¿æ¢æˆ[MASK]ï¼›ä¿æŒåŸè¯æ±‡ï¼›éšæœºæ›¿æ¢æˆå¦å¤–ä¸€ä¸ªè¯ï¼‰ï¼Œå¹¶éåªå±€é™äºå•è¯æ›¿æ¢æˆ`[MASK]`æ ‡ç­¾çš„æƒ…å†µã€‚
æ›´è¯¦ç»†çš„è¯´æ˜åŠæ ·ä¾‹è¯·å‚è€ƒï¼š[#4](https://github.com/ymcui/Chinese-BERT-wwm/issues/4)**

åŒç†ï¼Œç”±äºè°·æ­Œå®˜æ–¹å‘å¸ƒçš„`BERT-base, Chinese`ä¸­ï¼Œä¸­æ–‡æ˜¯ä»¥**å­—**ä¸ºç²’åº¦è¿›è¡Œåˆ‡åˆ†ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ä¼ ç»ŸNLPä¸­çš„ä¸­æ–‡åˆ†è¯ï¼ˆCWSï¼‰ã€‚
æˆ‘ä»¬å°†å…¨è¯Maskçš„æ–¹æ³•åº”ç”¨åœ¨äº†ä¸­æ–‡ä¸­ï¼Œä½¿ç”¨äº†ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆåŒ…æ‹¬ç®€ä½“å’Œç¹ä½“ï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”ä½¿ç”¨äº†åˆ†è¯å·¥å…·ï¼Œå³å¯¹ç»„æˆåŒä¸€ä¸ª**è¯**çš„æ±‰å­—å…¨éƒ¨è¿›è¡ŒMaskã€‚

ä¸‹è¿°æ–‡æœ¬å±•ç¤ºäº†`å…¨è¯Mask`çš„ç”Ÿæˆæ ·ä¾‹ã€‚
**æ³¨æ„ï¼šä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œä¸‹è¿°ä¾‹å­ä¸­åªè€ƒè™‘æ›¿æ¢æˆ[MASK]æ ‡ç­¾çš„æƒ…å†µã€‚**

| è¯´æ˜       | æ ·ä¾‹ |
|:---------| :--------- |
| åŸå§‹æ–‡æœ¬     | ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„probabilityã€‚ |
| åˆ†è¯æ–‡æœ¬     | ä½¿ç”¨ è¯­è¨€ æ¨¡å‹ æ¥ é¢„æµ‹ ä¸‹ ä¸€ä¸ª è¯ çš„ probability ã€‚ |
| åŸå§‹Maskè¾“å…¥ | ä½¿ ç”¨ è¯­ è¨€ [MASK] å‹ æ¥ [MASK] æµ‹ ä¸‹ ä¸€ ä¸ª è¯ çš„ pro [MASK] ##lity ã€‚ |
| å…¨è¯Maskè¾“å…¥ | ä½¿ ç”¨ è¯­ è¨€ [MASK] [MASK] æ¥ [MASK] [MASK] ä¸‹ ä¸€ ä¸ª è¯ çš„ [MASK] [MASK] [MASK] ã€‚ |



## æ¨¡å‹ä¸‹è½½

| æ•°æ®é›†               | owner      | model                                                     | è¯­è¨€     | å±‚æ•° | hidden | head | å‚æ•°é‡             |
|-------------------|------------|-----------------------------------------------------------|--------|----|--------|------|-----------------|
| æ–°é—»[corpus-3]      | Brian Shen | [bert_tiny_cn_tf],[bert_tiny_cn_pt]                       | cn     | 2  | 128    | 2    | 3.2M            |
| æ–°é—»[corpus-3]      | Brian Shen | [bert_mini_cn_tf], [bert_mini_cn_pt]                      | cn     | 4  | 256    | 4    | 8.8M            |
| ä¸­å­¦é˜…è¯»ç†è§£            | Brian Shen | [bert_2L_cn]                                              | cn     | 2  | 768    | 4    | 16.8M           |
| ä¸­å­¦é˜…è¯»ç†è§£            | Brian Shen | [bert_6L_cn]                                              | cn     | 6  | 768    | 12   | 45.1M           |
| ä¸­æ–‡ç»´åŸº              | Google     | [chinese_L-12_H-768_A-12_tf],[chinese_L-12_H-768_A-12_pt] | cn     | 12 | 768    | 12   | 102.3M[model-1] |
| ä¸­æ–‡ç»´åŸº              | Brian Shen | [bert_tywin_12L_cn]                                       | cn     | 12 | 768    | 12   | 102.3M          |
| ä¸­æ–‡ç»´åŸº              | Brian Shen | [bert_tyrion_12L_cn]                                      | cn     | 12 | 768    | 12   | 102.3M          |
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­” | Brian Shen | [bert-3L_cn-alpha]                                     | cn     | 3  | 768    | 12   | 38.5M           |
| ä¸­å­¦é˜…è¯»ç†è§£            | Brian Shen | [bert-3L_cn-beta]                                      | cn     | 3  | 1024   | 16   | 61.0M           |
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­” | Brian Shen | [bert_sansa_12L_cn]                                       | cn     | 12 | 768    | 12   | 102.3M          |
| ä¸­æ–‡è¯„è®º              | Brian Shen | [bert_eddard_12L_cn_tf],[bert_eddard_12L_cn_pt]           | cn     | 12 | 768    | 12   | 102.3M          |
| ä¸­æ–‡è¯„è®º              | Brian Shen | [bert_lyarra_12L_cn_tf],[bert_lyarra_12L_cn_pt]           | cn     | 12 | 768    | 12   | 105.1M          |
| ä¸­æ–‡è¯„è®º              | Brian Shen | [bert_rickard_12L_cn_tf],[bert_rickard_12L_cn_pt]         | cn     | 12 | 768    | 12   | 105.1M          |
| ä¸­æ–‡è¯„è®º              | Brian Shen | [bert_lyanna_12L_cn_tf],[bert_lyanna_12L_cn_pt]           | cn     | 12 | 768    | 12   | 105.1M          |
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­” | Brian Shen | [bert_24L_cn]                                             | cn     | 24 | 1024   | 16   | 325.5M          |
| QA                | Brian Shen | [bert_arya_24L_cn]                                        | cn     | 24 | 1024   | 16   | 325.5M          |
| QA                | Brian Shen | [bert_daenerys_24L_cn]                                    | cn     | 24 | 1024   | 16   | 325.5M          |
| æ–°é—»[corpus-4]      | Brian Shen | [bert_night-king_36L_cn_tf],[bert_night-king_36L_cn_pt]   | cn     | 36 | 1024   | 16   | 476.7M          |
| è‹±æ–‡æ–‡æœ¬              | Brian Shen | [stsb_drogo_L-12_H-768_A-12]                                     | en     | 12 | 768    | 12   | 109.5M          |
| è‹±æ–‡æ–‡æœ¬              | Brian Shen | [stsb_L-12_H-768_A-12]                                    | en     | 12 | 768    | 12   | 124.6M          |
| è‹±æ–‡æ–‡æœ¬              | Brian Shen | [stsb_L-24_H-1024_A-16]                                   | en     | 24 | 1024   | 16   | 355.3M          |
| å¤šè¯­è¨€è¯­æ–™             | Brian Shen | [stsb-multi_L-12_H-768_A-12]                              | global | 12 | 768    | 12   | 278M               |


> **`base`**ï¼š12-layer, 768-hidden, 12-heads, 102.3M parameters  
> **`large`**ï¼š24-layer, 1024-hidden, 16-heads, 325.5M parameters
> **`giant`**ï¼š36-layer, 1024-hidden, 16-heads, 476.7M parameters

> [corpus-1] é€šç”¨æ•°æ®åŒ…æ‹¬ï¼šé—®ç­”ç­‰æ•°æ®ï¼Œæ€»å¤§å°12.5MBï¼Œè®°å½•æ•°1ä¸‡ï¼Œå­—æ•°7.2ä¸‡ã€‚  
> [corpus-2] åŠ è½½pytorchå’Œtf2æ¨¡å‹æ—¶ï¼Œå¦‚transformersåŠ è½½æŠ¥xlaé”™è¯¯ï¼Œè¯·è‡ªè¡Œä¿®æ”¹config.jsonä¸­`xla_device`çš„å€¼ï¼Œå¦‚åœ¨gpuä¸Šå¾®è°ƒéœ€è¦è®¾ä¸ºfalseï¼Œå¦‚åœ¨tpuä¸Šå¾®è°ƒï¼Œåˆ™éœ€è¦è®¾ä¸ºtrueã€‚  
> [corpus-3] æ–°é—»è¯­æ–™ï¼š5000ç¯‡2021å¹´æ–°é—»ï¼Œå¤§å°çº¦13MBã€‚  
> [corpus-4] æ–°é—»è¯­æ–™ï¼šå¤šç¯‡2021å¹´æ–°é—»ï¼Œå¤§å°çº¦200GBã€‚
> [model-1] Chinese-Bert-Base: ä¸­æ–‡BERT-Baseçš„å‚æ•°é‡ç»è®¡ç®—ä¸º102.3Mï¼Œè€Œè°·æ­Œè‹±æ–‡BERT-Baseå‚æ•°é‡ä¸º110Mï¼Œå…¶å·®è·åº”ä¸ºè¯è¡¨æ•°ä¸ä¸€è‡´å¯¼è‡´çš„ã€‚ç»Ÿè®¡è„šæœ¬[count.py](/count.py)ã€‚

 


[bert_tiny_cn_tf]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/tf1/chinese_L-2_H-128_A-2.zip
[bert_tiny_cn_pt]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/chinese_L-2_H-128_A-2.zip
[bert_mini_cn_tf]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/tf1/chinese_L-4_H-256_A-4.zip
[bert_mini_cn_pt]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/chinese_L-4_H-256_A-4.zip
[bert_2L_cn]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/bert_L-2_H-768_A-4_cn.zip
[bert_6L_cn]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/bert_L-6_H-768_A-12_cn.zip
[chinese_L-12_H-768_A-12_tf]: https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip
[chinese_L-12_H-768_A-12_pt]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/chinese_L-12_H-768_A-12.tgz
[bert_tywin_12L_cn]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/bert_tywin_L-12_H-768_A-12_cn.zip
[bert_tyrion_12L_cn]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/bert_tyrion_L-12_H-768_A-12_cn.zip
[bert-3L_cn-alpha]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_L-3_H-768_A-12_cn.zip
[bert-3L_cn-beta]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_L-3_H-1024_A-16_cn.zip
[bert_sansa_12L_cn]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_sansa_L-12_H-768_A-12_cn.zip
[bert_eddard_12L_cn_tf]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/tf1/roberta_eddard_L-12_H-768_A-12_cn.zip
[bert_eddard_12L_cn_pt]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_eddard_L-12_H-768_A-12_cn.zip
[bert_lyarra_12L_cn_tf]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/tf1/roberta_lyarra_L-12_H-768_A-12_cn_tf.zip
[bert_lyarra_12L_cn_pt]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_lyarra_L-12_H-768_A-12_cn.zip
[bert_rickard_12L_cn_tf]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/tf1/roberta_rickard_L-12_H-768_A-12_cn_tf.zip
[bert_rickard_12L_cn_pt]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_rickard_L-12_H-768_A-12_cn.zip
[bert_lyanna_12L_cn_tf]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/tf1/roberta_lyanna_L-12_H-768_A-12_cn_tf.zip
[bert_lyanna_12L_cn_pt]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_lyanna_L-12_H-768_A-12_cn.zip
[bert_24L_cn]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_L-24_H-1024_A-16_cn.zip
[bert_arya_24L_cn]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_arya_L-24_H-1024_A-16_cn.zip
[bert_daenerys_24L_cn]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_daenerys_L-24_H-1024_A-16_cn.zip
[bert_night-king_36L_cn_tf]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/tf1/roberta_night-king_L-36_H-1024_A-16_cn_tf.zip
[bert_night-king_36L_cn_pt]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/roberta_night-king_L-36_H-1024_A-16_cn.zip
[stsb_drogo_L-12_H-768_A-12]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/stsb_drogo_L-12_H-768_A-12.zip
[stsb_L-12_H-768_A-12]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/stsb_L-12_H-768_A-12.zip
[stsb_L-24_H-1024_A-16]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/stsb_L-24_H-1024_A-16.zip
[stsb-multi_L-12_H-768_A-12]: https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/pt/stsb-multi_L-12_H-768_A-12.zip


### PyTorch/Tensorflowç‰ˆæœ¬

éƒ¨åˆ†æä¾›Tensorflowå’ŒPytorchç‰ˆæœ¬ï¼Œéƒ¨åˆ†ä»…æä¾›PyTorchç‰ˆæœ¬ã€‚

### ä½¿ç”¨è¯´æ˜

`bert_12L_cn`æ¨¡å‹æ–‡ä»¶å¤§å°çº¦**454M**å’Œ**1.3G**ã€‚

Pytorchç‰ˆæœ¬ä¸ºï¼š

```
chinese_BERT_base_L-12_H-768_A-12.zip
    |- pytorch_model.bin     # æ¨¡å‹æƒé‡
    |- config.json           # æ¨¡å‹å‚æ•°
    |- vocab.txt             # åˆ†è¯è¯è¡¨
```


`stsb`æ¨¡å‹éœ€è¦ä½¿ç”¨`sentence_transformers`åº“åŠ è½½ï¼Œå…ˆ`pip install sentence_transformers`å®‰è£…åä½¿ç”¨ã€‚  
```python
from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('stsb_L-24_H-1024_A-16')
embeddings = model.encode(sentences)
print(embeddings)
```

### å¿«é€ŸåŠ è½½

ä¾æ‰˜äº[Huggingface-Transformers 3.1.0](https://github.com/huggingface/transformers) ï¼Œå¯è½»æ¾è°ƒç”¨ä»¥ä¸Šæ¨¡å‹ã€‚
```
tokenizer = AutoTokenizer.from_pretrained("MODEL_NAME")
model = AutoModel.from_pretrained("MODEL_NAME")

æˆ–

tokenizer = BertTokenizer.from_pretrained("MODEL_NAME")
model = BertModel.from_pretrained("MODEL_NAME")
```



## åŸºçº¿ç³»ç»Ÿæ•ˆæœ
ä¸ºäº†å¯¹æ¯”åŸºçº¿æ•ˆæœï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹å‡ ä¸ªè‹±æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚å¯¹æ¯”äº†è‹±æ–‡BERT-Tinyã€ä¸­æ–‡BERT-Tinyã€ä¸­æ–‡BERT-Miniã€ä¸­æ–‡BERT-wwm-extã€BERT-baseä»¥åŠæœ¬é¡¹ç›®çš„bert_12L_cnã€‚

| Model        | Score |  CoLA  | SST-2 |  MRPC   | STS-B |  QQP  |MNLI-m| MNLI-mm |QNLI(v2)|  RTE  | WNLI  |
|--------------|:-----:|:------:|:-----:|:-------:|:-----:|:-----:|:----:|:-------:|:------:|:-----:|:-----:|
| BERT-Tiny[1] | 65.13 | 69.12  | 79.12 |  70.34  | 42.73 | 79.81 |64.60 |  66.47  | 77.63  | 59.21 | 42.25 |
| BERT-Mini[1] | 65.93 | 69.12	 | 83.60 | 	72.79  | 45.27 | 76.01 |71.42 |  56.06  | 83.21  | 61.01 | 40.85 |
| BERT-Tiny-CN | 56.00 | 69.12  | 71.10 | 	68.38  | 24.33 | 73.79 |49.23 |  49.79  | 59.30  | 51.26 | 43.66 |
| BERT-Mini-CN | 58.70 | 69.12	 | 75.91 | 	68.38  | 25.40 | 76.09 |55.24 |  55.09  | 56.33  | 49.10 | 56.34 |

> [1] è¿™æ˜¯è°·æ­Œçš„BERT-Tiny/Miniæ¨¡å‹ï¼Œåœ¨GLUEæµ‹è¯•æ•°æ®é›†ä¸Šï¼ŒCoLAè¯„åˆ†ä¸º0ï¼Œæœ¬æµ‹è¯„ä½¿ç”¨ç›¸åŒè„šæœ¬é‡æ–°å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œç»Ÿä¸€æµ‹è¯„ï¼Œä»¥å¯¹æ¯”ç»“æœã€‚

æ¯ä¸ªä»»åŠ¡æˆ‘ä»¬é‡‡ç”¨ç›¸åŒçš„è®­ç»ƒå‚æ•°è®­ç»ƒä¸€è½®ï¼Œå…¶ä»–å‚æ•°å¦‚ä¸‹ï¼š
* max seq length: 128
* batch size: 4
* learning rate: 2e-5

> ç»“è®ºï¼Œåˆ©ç”¨æ–°é—»è¯­æ–™[corpus-3]è®­ç»ƒå¾—åˆ°çš„ä¸­æ–‡BERT-Tiny/Miniï¼Œåœ¨GLUEæ•°æ®é›†ä¸Šï¼Œä¸è°·æ­Œç»™å‡ºçš„BERT-Tiny/Miniç›¸æ¯”å¾—åˆ°äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ 
> ä¸è°·æ­ŒBERT-Tiny/Miniç›¸æ¯”: ä¸­æ–‡BERT-Tiny(-9.13%/-9.93%)ï¼Œä¸­æ–‡BERT-Mini(-6.43%/-7.23%);
> ç”±äºè¿™ä¸¤ä¸ªæ¨¡å‹æ˜¯åœ¨ä¸­æ–‡è¯­æ–™ä¸Šè®­ç»ƒè€Œæˆï¼Œå…¶åœ¨è‹±æ–‡GLUEæµ‹è¯„ä¸Šå–å¾—æ­¤æ•ˆæœï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨è‹±æ–‡ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚
> åˆ†æä¸ºä½•ä¸­æ–‡æ¨¡å‹èƒ½åœ¨è‹±æ–‡ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œç›–å› ä¸ºæ­¤ä¸­æ–‡æ¨¡å‹ä½¿ç”¨çš„è°·æ­Œå®˜æ–¹ä¸­æ–‡æ¨¡å‹çš„21Kä¸ªè¯çš„è¯è¡¨ï¼Œå…¶ä¸­åŒ…å«äº†å¤§é‡çš„å¸¸è§è‹±æ–‡è¯æ±‡ï¼Œå› æ­¤å…·å¤‡è¡¨å¾è‹±æ–‡æ–‡æœ¬çš„æ½œåŠ›ã€‚ ä½†ç”±äºæ­¤æ¨¡å‹ä¸æ˜¯ä¸“ä¸ºè‹±æ–‡è®¾è®¡ï¼Œå› æ­¤ä¸è°·æ­ŒBERT-Tinyæ¨¡å‹ç›¸æ¯”ï¼Œå…¶è¡¨å¾èƒ½åŠ›åˆ†åˆ«å·®äº†9.13%/9.93%ã€6.43%/7.23%ã€‚

| Model        | Score | SQUAD 1.1 | SQUAD-2 |
|--------------|:-----:|:---------:|:-------:|
| BERT-Tiny    | 45.27 |   39.88   |  50.66  |
| BERT-Mini    | 64.03 |   68.58   |  59.47  |
| BERT-Tiny-CN | 29.78 |   9.48    |  50.07  |
| BERT-Mini-CN | 31.76 |  13.45    |  50.06  |

æ¯ä¸ªä»»åŠ¡æˆ‘ä»¬é‡‡ç”¨ç›¸åŒçš„è®­ç»ƒå‚æ•°è®­ç»ƒäºŒè½®ï¼Œå…¶ä»–å‚æ•°å¦‚ä¸‹ï¼š
* max seq length: 384
* batch size: 12
* learning rate: 3e-5
* doc stride: 128

ç»“è®ºï¼Œç”±äºBERT-Tiny/Mini-CNæ˜¯ä¸­æ–‡è¯­æ–™è®­ç»ƒçš„ï¼Œåœ¨è‹±æ–‡é˜…è¯»ç†è§£/é—®ç­”ä»»åŠ¡ä¸­æ¯”Googleçš„BERT-Tiny/Miniæ•ˆæœå·®15%-33%ã€‚

## é¢„è®­ç»ƒåˆ†è¯
`BERT-Tiny-CN`å’Œ`BERT-Mini-CN`è¿™ä¸¤ä¸ªæ¨¡å‹é‡‡ç”¨ä¸­æ–‡æŒ‰å­—åˆ†è¯ï¼Œä¸è¿›è¡Œå°å†™è½¬æ¢ã€‚

æŒ‰è¯MASKç­‰æ¨¡å‹çš„ç»†èŠ‚ï¼Œåˆ™ä»¥`bert_12L_cn`æ¨¡å‹ä¸ºä¾‹ï¼Œå¯¹é¢„è®­ç»ƒç»†èŠ‚è¿›è¡Œè¯´æ˜ã€‚

### ç”Ÿæˆè¯è¡¨

æŒ‰ç…§BERTå®˜æ–¹æ•™ç¨‹æ­¥éª¤ï¼Œé¦–å…ˆéœ€è¦ä½¿ç”¨[Word Piece](https://pypi.org/project/tokenizers/) ç”Ÿæˆè¯è¡¨ã€‚
WordPieceæ˜¯ç”¨äºBERTã€DistilBERTå’ŒElectraçš„å­è¯æ ‡è®°åŒ–ç®—æ³•ã€‚è¯¥ç®—æ³•åœ¨æ—¥è¯­å’ŒéŸ©è¯­è¯­éŸ³æœç´¢ï¼ˆSchuster et al.ï¼Œ2012ï¼‰ä¸­è¿›è¡Œäº†æ¦‚è¿°ï¼Œä¸BPEéå¸¸ç›¸ä¼¼ã€‚WordPieceé¦–å…ˆåˆå§‹åŒ–è¯æ±‡è¡¨ä»¥åŒ…å«è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸ªå­—ç¬¦ï¼Œå¹¶é€æ­¥å­¦ä¹ ç»™å®šæ•°é‡çš„åˆå¹¶è§„åˆ™ã€‚ä¸BPEä¸åŒçš„æ˜¯ï¼ŒWordPieceå¹¶æ²¡æœ‰é€‰æ‹©æœ€é¢‘ç¹çš„ç¬¦å·å¯¹ï¼Œè€Œæ˜¯é€‰æ‹©å°†è®­ç»ƒæ•°æ®æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„å¯èƒ½æ€§æœ€å¤§åŒ–çš„ç¬¦å·å¯¹ã€‚
é‚£ä¹ˆè¿™åˆ°åº•æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿå‚ç…§å‰é¢çš„ç¤ºä¾‹ï¼Œæœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„å¯èƒ½æ€§ç›¸å½“äºæ‰¾åˆ°ç¬¦å·å¯¹ï¼Œå…¶æ¦‚ç‡é™¤ä»¥å…¶ç¬¬ä¸€ä¸ªç¬¦å·çš„æ¦‚ç‡ç„¶åå†é™¤ä»¥å…¶ç¬¬äºŒä¸ªç¬¦å·çš„æ¦‚ç‡æ˜¯æ‰€æœ‰ç¬¦å·å¯¹ä¸­æœ€å¤§çš„ã€‚Eã€ åªæœ‰å½“â€œugâ€é™¤ä»¥â€œuâ€ã€â€œgâ€çš„æ¦‚ç‡å¤§äºä»»ä½•å…¶ä»–ç¬¦å·å¯¹æ—¶ï¼Œâ€œuâ€åæ¥â€œgâ€æ‰ä¼šåˆå¹¶ã€‚ç›´è§‚åœ°è¯´ï¼ŒWordPieceä¸BPEç¨æœ‰ä¸åŒï¼Œå®ƒé€šè¿‡åˆå¹¶ä¸¤ä¸ªç¬¦å·æ¥è¯„ä¼°å®ƒæ‰€å¤±å»çš„ä¸œè¥¿ï¼Œä»¥ç¡®ä¿å®ƒæ˜¯å€¼å¾—çš„ã€‚

åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„è¯è¡¨å¤§å°ä¸º21128ï¼Œå…¶ä½™å‚æ•°é‡‡ç”¨å®˜æ–¹ç¤ºä¾‹ä¸­çš„é»˜è®¤é…ç½®ã€‚

```
# Initialize a tokenizer
tokenizer = BertWordPieceTokenizer()
# Customize training
tokenizer.train(files=paths, vocab_size=21_128, min_frequency=0, special_tokens=[
                "[PAD]",
                "[UNK]",
                "[CLS]",
                "[SEP]",
                "[MASK]",
                "<S>",
                "<T>"
                ])
```


### ç”Ÿæˆè¯è¡¨ç®—æ³•

ä¸‹åˆ—æ–¹æ³•å¹¶étokenizersçš„å®ç°ï¼Œä½œä¸ºæ„æ€è¡¨è¾¾ã€‚è¿›ä¸€æ­¥åœ°ï¼Œå¯¹äºä¸€ä¸ªè‹±æ–‡è¯ï¼ˆä¸­æ–‡åˆ†è¯åŒç†ï¼‰ï¼ŒæŒ‰ç…§WPè§„åˆ™ï¼Œå¯åˆ†æˆå¤šä¸ªé«˜é¢‘ç‰‡æ®µã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```
def tokenize(self, text):
  
  # æŠŠä¸€æ®µæ–‡å­—åˆ‡åˆ†æˆword pieceã€‚è¿™å…¶å®æ˜¯è´ªå¿ƒçš„æœ€å¤§æ­£å‘åŒ¹é…ç®—æ³•ã€‚
  # æ¯”å¦‚ï¼š
  # input = "unaffable"
  # output = ["un", "##aff", "##able"]
 
  
  text = convert_to_unicode(text)
  
  output_tokens = []
  for token in whitespace_tokenize(text):
	  chars = list(token)
	  if len(chars) > self.max_input_chars_per_word:
		  output_tokens.append(self.unk_token)
		  continue
	  
	  is_bad = False
	  start = 0
	  sub_tokens = []
	  while start < len(chars):
		  end = len(chars)
		  cur_substr = None
		  while start < end:
			  substr = "".join(chars[start:end])
			  if start > 0:
				  substr = "##" + substr
			  if substr in self.vocab:
				  cur_substr = substr
				  break
			  end -= 1
		  if cur_substr is None:
			  is_bad = True
			  break
		  sub_tokens.append(cur_substr)
		  start = end
	  
	  if is_bad:
		  output_tokens.append(self.unk_token)
	  else:
		  output_tokens.extend(sub_tokens)
  return output_tokens
```



### é¢„è®­ç»ƒ

`BERT-Tiny-CN`å’Œ`BERT-Mini-CN`ä¸¤ä¸ªæ¨¡å‹çš„è®­ç»ƒå‚æ•°ä¸ºï¼š
*  train_batch_size: 32
*  max_seq_length: 128
*  max_predictions_per_seq: 20
*  num_train_steps: 100000
*  num_warmup_steps: 5000
*  learning_rate: 2e-5

è®­ç»ƒç»“æœå¦‚ä¸‹:  
* BERT-Tiny: masked_lm_accuracy=22.74%ï¼ŒNSP_accuracy=100%ã€‚  
BERT-Mini: masked_lm_accuracy=33.54%ï¼ŒNSP_accuracy=100%ã€‚  

è·å¾—ä»¥ä¸Šæ•°æ®åï¼Œæˆªæ­¢2021å¹´2æœˆ6æ—¥ï¼Œä½¿ç”¨BERT-wwm-extçš„WordPieceè¯è¡¨ï¼ˆæ¨¡å‹ï¼‰ï¼Œ(æœªæ¥å°†ä½¿ç”¨åŸºäºé€šç”¨æ•°æ®çš„WordPieceæ¨¡å‹)ï¼Œæ­£å¼å¼€å§‹é¢„è®­ç»ƒBERTã€‚
ä¹‹æ‰€ä»¥å«`bert_12L_cn`æ˜¯å› ä¸ºä»…ç›¸æ¯”`BERT-wwm-ext`ï¼Œå…¶ä½™å‚æ•°æ²¡æœ‰å˜åŠ¨ï¼Œä¸»è¦å› ä¸ºè®¡ç®—è®¾å¤‡å—é™ã€‚
ä½¿ç”¨çš„å‘½ä»¤å¦‚ä¸‹ï¼š
```
    from transformers import (
        CONFIG_MAPPING,
        MODEL_WITH_LM_HEAD_MAPPING,
        AutoConfig,
        BertConfig,
        RobertaConfig,
        AutoModelWithLMHead,
        BertForMaskedLM,
        RobertaForMaskedLM,
        AutoTokenizer,
        DataCollatorForLanguageModeling,
        HfArgumentParser,
        LineByLineTextDataset,
        PreTrainedTokenizer,
        TextDataset,
        Trainer,
        TrainingArguments,
        set_seed,
        BertTokenizer
    )
   
    TEMP="temp/"
    
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-c', '--config_file', default='config/bert_config.json',
        help='model config file')
    
    args = parser.parse_args()
    with open(args.config_file) as fin:
        config = json.load(fin, object_hook=lambda d: SimpleNamespace(**d))
    
    bert_config = BertConfig.from_pretrained(config.bert_model_path, cache_dir=TEMP)
    
    WRAPPED_MODEL = BertForMaskedLM.from_pretrained(
                config.bert_model_path,
                from_tf=False,
                config=bert_config,
                cache_dir=TEMP,
            )
    for param in WRAPPED_MODEL.parameters():
        param.requires_grad = True
    WRAPPED_MODEL.train()
    
    tokenizer = BertTokenizer.from_pretrained(config.bert_model_path)
    WRAPPED_MODEL.resize_token_embeddings(len(tokenizer))
    
    print("dataset maxl:", config.max_seq_len)
    
    dataset = LineByLineTextDataset(
        tokenizer=tokenizer,
        file_path=config.train_file_path,
        block_size=config.max_seq_len,
    )
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=0.15
    )
    
    """### Finally, we are all set to initialize our Trainer"""
    training_args = TrainingArguments(
        output_dir=TEMP,
        overwrite_output_dir=True,
        num_train_epochs=1,
        per_device_train_batch_size=config.batch_size,
        save_steps=10_000,
        save_total_limit=2,
        tpu_num_cores=8,
    )
    
    trainer = Trainer(
        model=WRAPPED_MODEL,
        args=training_args,
        data_collator=data_collator,
        train_dataset=dataset,
        prediction_loss_only=True,
    )
    
    
    trainer.train(model_path=config.bert_model_path)
    WRAPPED_MODEL.to('cpu')
    trainer.save_model(output_dir=config.trained_model_path)
    torch.save(WRAPPED_MODEL.state_dict(), os.path.join(config.trained_model_path, 'pytorch_model.bin'))
```

## ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒç»†èŠ‚
ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒä½¿ç”¨çš„è®¾å¤‡æ˜¯è°·æ­ŒCloud GPUï¼ˆ16G HBMï¼‰ï¼Œä»¥ä¸‹ç®€è¦è¯´æ˜å„ä»»åŠ¡ç²¾è°ƒæ—¶çš„é…ç½®ã€‚
**ç›¸å…³ä»£ç è¯·æŸ¥çœ‹é¡¹ç›®ã€‚**


## FAQ
**Q: è¿™ä¸ªæ¨¡å‹æ€ä¹ˆç”¨ï¼Ÿ**  
A: è°·æ­Œå‘å¸ƒçš„ä¸­æ–‡BERTæ€ä¹ˆç”¨ï¼Œè¿™ä¸ªå°±æ€ä¹ˆç”¨ã€‚
**æ–‡æœ¬ä¸éœ€è¦ç»è¿‡åˆ†è¯ï¼Œwwmåªå½±å“é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å½±å“ä¸‹æ¸¸ä»»åŠ¡çš„è¾“å…¥ã€‚**

**Q: è¯·é—®æœ‰é¢„è®­ç»ƒä»£ç æä¾›å—ï¼Ÿ**  
A: å¾ˆé—æ†¾ï¼Œæˆ‘ä¸èƒ½æä¾›ç›¸å…³ä»£ç ï¼Œå®ç°å¯ä»¥å‚è€ƒ [#10](https://github.com/ymcui/Chinese-BERT-wwm/issues/10) å’Œ [#13](https://github.com/ymcui/Chinese-BERT-wwm/issues/13)ã€‚

**Q: æŸæŸæ•°æ®é›†åœ¨å“ªé‡Œä¸‹è½½ï¼Ÿ**  
A: è¯·æŸ¥çœ‹`data`ç›®å½•ï¼Œä»»åŠ¡ç›®å½•ä¸‹çš„`README.md`æ ‡æ˜äº†æ•°æ®æ¥æºã€‚å¯¹äºæœ‰ç‰ˆæƒçš„å†…å®¹ï¼Œè¯·è‡ªè¡Œæœç´¢æˆ–ä¸åŸä½œè€…è”ç³»è·å–æ•°æ®ã€‚

**Q: ä¼šæœ‰è®¡åˆ’å‘å¸ƒæ›´å¤§æ¨¡å‹å—ï¼Ÿæ¯”å¦‚BERT-large-wwmç‰ˆæœ¬ï¼Ÿ**  
A: å¦‚æœæˆ‘ä»¬ä»å®éªŒä¸­å¾—åˆ°æ›´å¥½æ•ˆæœï¼Œä¼šè€ƒè™‘å‘å¸ƒæ›´å¤§çš„ç‰ˆæœ¬ã€‚

**Q: ä½ éª—äººï¼æ— æ³•å¤ç°ç»“æœğŸ˜‚**  
A: åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€ç®€å•çš„æ¨¡å‹ã€‚æ¯”å¦‚åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨çš„æ˜¯`run_classifier.py`ï¼ˆè°·æ­Œæä¾›ï¼‰ã€‚
å¦‚æœæ— æ³•è¾¾åˆ°å¹³å‡å€¼ï¼Œè¯´æ˜å®éªŒæœ¬èº«å­˜åœ¨bugï¼Œè¯·ä»”ç»†æ’æŸ¥ã€‚
æœ€é«˜å€¼å­˜åœ¨å¾ˆå¤šéšæœºå› ç´ ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯èƒ½å¤Ÿè¾¾åˆ°æœ€é«˜å€¼ã€‚
å¦å¤–ä¸€ä¸ªå…¬è®¤çš„å› ç´ ï¼šé™ä½batch sizeä¼šæ˜¾è‘—é™ä½å®éªŒæ•ˆæœï¼Œå…·ä½“å¯å‚è€ƒBERTï¼ŒXLNetç›®å½•çš„ç›¸å…³Issueã€‚

**Q: æˆ‘è®­å‡ºæ¥æ¯”ä½ æ›´å¥½çš„ç»“æœï¼**  
A: æ­å–œä½ ã€‚

**Q: è®­ç»ƒèŠ±äº†å¤šé•¿æ—¶é—´ï¼Œåœ¨ä»€ä¹ˆè®¾å¤‡ä¸Šè®­ç»ƒçš„ï¼Ÿ**  
A: è®­ç»ƒæ˜¯åœ¨è°·æ­ŒTPU v3ç‰ˆæœ¬ï¼ˆ128G HBMï¼‰å®Œæˆçš„ï¼Œè®­ç»ƒBERT-wwm-baseèŠ±è´¹çº¦4å°æ—¶ï¼ŒBERT-wwm-largeåˆ™èŠ±è´¹çº¦8å°æ—¶ã€‚

**Q: BERT-wwmçš„æ•ˆæœä¸æ˜¯åœ¨æ‰€æœ‰ä»»åŠ¡éƒ½å¾ˆå¥½**  
A: æœ¬é¡¹ç›®çš„ç›®çš„æ˜¯ä¸ºç ”ç©¶è€…æä¾›å¤šå…ƒåŒ–çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè‡ªç”±é€‰æ‹©BERTï¼ŒERNIEï¼Œæˆ–è€…æ˜¯BERT-wwmã€‚
æˆ‘ä»¬ä»…æä¾›å®éªŒæ•°æ®ï¼Œå…·ä½“æ•ˆæœå¦‚ä½•è¿˜æ˜¯å¾—åœ¨è‡ªå·±çš„ä»»åŠ¡ä¸­ä¸æ–­å°è¯•æ‰èƒ½å¾—å‡ºç»“è®ºã€‚
å¤šä¸€ä¸ªæ¨¡å‹ï¼Œå¤šä¸€ç§é€‰æ‹©ã€‚

**Q: ä¸ºä»€ä¹ˆæœ‰äº›æ•°æ®é›†ä¸Šæ²¡æœ‰è¯•ï¼Ÿ**  
A: å¾ˆå¦ç‡çš„è¯´ï¼š
1ï¼‰æ²¡ç²¾åŠ›æ‰¾æ›´å¤šçš„æ•°æ®ï¼›
2ï¼‰æ²¡æœ‰å¿…è¦ï¼› 
3ï¼‰æ²¡æœ‰é’ç¥¨ï¼›

**Q: ç®€å•è¯„ä»·ä¸€ä¸‹è¿™å‡ ä¸ªæ¨¡å‹**  
A: å„æœ‰ä¾§é‡ï¼Œå„æœ‰åƒç§‹ã€‚
ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶å‘å±•éœ€è¦å¤šæ–¹å…±åŒåŠªåŠ›ã€‚

**Q: æ›´å¤šå…³äº`RoBERTa-wwm-ext`æ¨¡å‹çš„ç»†èŠ‚ï¼Ÿ**  
A: æˆ‘ä»¬é›†æˆäº†RoBERTaå’ŒBERT-wwmçš„ä¼˜ç‚¹ï¼Œå¯¹ä¸¤è€…è¿›è¡Œäº†ä¸€ä¸ªè‡ªç„¶çš„ç»“åˆã€‚
å’Œä¹‹å‰æœ¬ç›®å½•ä¸­çš„æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«å¦‚ä¸‹:  
1ï¼‰é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨wwmç­–ç•¥è¿›è¡Œmaskï¼ˆä½†æ²¡æœ‰ä½¿ç”¨dynamic maskingï¼‰  
2ï¼‰ç®€å•å–æ¶ˆNext Sentence Predictionï¼ˆNSPï¼‰loss  
3ï¼‰ä¸å†é‡‡ç”¨å…ˆmax_len=128ç„¶åå†max_len=512çš„è®­ç»ƒæ¨¡å¼ï¼Œç›´æ¥è®­ç»ƒmax_len=512  
4ï¼‰è®­ç»ƒæ­¥æ•°é€‚å½“å»¶é•¿  

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹å¹¶éåŸç‰ˆRoBERTaæ¨¡å‹ï¼Œåªæ˜¯æŒ‰ç…§ç±»ä¼¼RoBERTaè®­ç»ƒæ–¹å¼è®­ç»ƒå‡ºçš„BERTæ¨¡å‹ï¼Œå³RoBERTa-like BERTã€‚
æ•…åœ¨ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ã€æ¨¡å‹è½¬æ¢æ—¶è¯·æŒ‰BERTçš„æ–¹å¼å¤„ç†ï¼Œè€ŒéRoBERTaã€‚


## å¼•ç”¨
å¦‚æœæœ¬ç›®å½•ä¸­çš„å†…å®¹å¯¹ä½ çš„ç ”ç©¶å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿åœ¨è®ºæ–‡ä¸­å¼•ç”¨ä¸‹è¿°æŠ€æœ¯æŠ¥å‘Šï¼š


## è‡´è°¢
é¡¹ç›®ä½œè€…ï¼š Brian Shen. Twitter @dezhou.

å»ºè®¾è¯¥é¡¹ç›®è¿‡ç¨‹ä¸­å‚è€ƒäº†å¦‚ä¸‹ä»“åº“ï¼Œåœ¨è¿™é‡Œè¡¨ç¤ºæ„Ÿè°¢ï¼š
- BERTï¼šhttps://github.com/google-research/bert
- ä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹ï¼šhttps://github.com/ymcui/Chinese-BERT-wwm


## å…è´£å£°æ˜
æœ¬é¡¹ç›®å¹¶é[BERTå®˜æ–¹](https://github.com/google-research/bert) å‘å¸ƒçš„Chinese BERTæ¨¡å‹ã€‚
è¯¥é¡¹ç›®ä¸­çš„å†…å®¹ä»…ä¾›æŠ€æœ¯ç ”ç©¶å‚è€ƒï¼Œä¸ä½œä¸ºä»»ä½•ç»“è®ºæ€§ä¾æ®ã€‚
ä½¿ç”¨è€…å¯ä»¥åœ¨è®¸å¯è¯èŒƒå›´å†…ä»»æ„ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨è¯¥é¡¹ç›®å†…å®¹é€ æˆçš„ç›´æ¥æˆ–é—´æ¥æŸå¤±è´Ÿè´£ã€‚


## å…³æ³¨æˆ‘ä»¬
æ¬¢è¿å…³æ³¨çŸ¥ä¹ä¸“æ å·ã€‚

[æ·±åº¦å­¦ä¹ å…´è¶£å°ç»„](https://www.zhihu.com/column/thuil)


## é—®é¢˜åé¦ˆ & è´¡çŒ®
å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚  
æˆ‘ä»¬æ²¡æœ‰è¿è¥ï¼Œé¼“åŠ±ç½‘å‹äº’ç›¸å¸®åŠ©è§£å†³é—®é¢˜ã€‚  
å¦‚æœå‘ç°å®ç°ä¸Šçš„é—®é¢˜æˆ–æ„¿æ„å…±åŒå»ºè®¾è¯¥é¡¹ç›®ï¼Œè¯·æäº¤Pull Requestã€‚
